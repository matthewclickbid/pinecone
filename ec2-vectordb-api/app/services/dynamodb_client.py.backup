import os
import uuid
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
from decimal import Decimal

logger = logging.getLogger(__name__)

# Check if we're in local mode
USE_LOCAL_FILES = os.environ.get('USE_LOCAL_FILES', 'false').lower() == 'true'
ENVIRONMENT = os.environ.get('ENVIRONMENT', 'development').lower()

if USE_LOCAL_FILES or ENVIRONMENT == 'local':
    # Use local storage for development/testing
    from app.services.local_storage import LocalStorageClient
    
    class DynamoDBClient(LocalStorageClient):
        """Use LocalStorageClient as DynamoDBClient in local mode"""
        pass
else:
    # Use actual DynamoDB in non-local environments
    import boto3
    from botocore.exceptions import ClientError

    class DynamoDBClient:
        def __init__(self):
            self.table_name = os.environ.get('DYNAMODB_TABLE_NAME', 'vectordb-tasks')
            self.dynamodb = boto3.resource('dynamodb')
            self.table = self.dynamodb.Table(self.table_name)
            
            logger.info(f"Initialized DynamoDB client for table: {self.table_name}")
            
            # Lock table for preventing concurrent processing of same date ranges
            self.lock_table_name = f"{self.table_name}-locks"
            self.lock_table = self.dynamodb.Table(self.lock_table_name)
        
        def _convert_decimal(self, obj):
        """Convert Decimal objects to native Python types for JSON serialization."""
        if isinstance(obj, list):
            return [self._convert_decimal(item) for item in obj]
        elif isinstance(obj, dict):
            return {key: self._convert_decimal(value) for key, value in obj.items()}
        elif isinstance(obj, Decimal):
            # Convert Decimal to int if it's a whole number, otherwise to float
            if obj % 1 == 0:
                return int(obj)
            else:
                return float(obj)
        else:
            return obj
    
            def _convert_to_dynamodb_format(self, obj):
        """Convert Python types to DynamoDB-compatible formats."""
        if isinstance(obj, float):
            # Convert float to Decimal for DynamoDB
            return Decimal(str(obj))
        elif isinstance(obj, list):
            return [self._convert_to_dynamodb_format(item) for item in obj]
        elif isinstance(obj, dict):
            return {key: self._convert_to_dynamodb_format(value) for key, value in obj.items()}
        else:
            return obj
    
            def create_task(self, start_date: str, end_date: str, total_records: int = 0, task_type: str = 'data_processing') -> str:
        """
        Create a new task in DynamoDB.
        
        Args:
            start_date (str): Start date for the task
            end_date (str): End date for the task
            total_records (int): Total number of records to process
            task_type (str): Type of task (data_processing, namespace_migration)
            
        Returns:
            str: Task ID
        """
        try:
            task_id = str(uuid.uuid4())
            timestamp = datetime.utcnow().isoformat()
            
            item = {
                'task_id': task_id,
                'status': 'PENDING',
                'task_type': task_type,
                'start_date': start_date,
                'end_date': end_date,
                'total_records': total_records,
                'processed_records': 0,
                'failed_records': 0,
                'created_at': timestamp,
                'updated_at': timestamp,
                'error_message': None
            }
            
            self.table.put_item(Item=item)
            logger.info(f"Created task {task_id} with status PENDING")
            
            return task_id
            
        except ClientError as e:
            logger.error(f"Error creating task: {e}")
            raise
    
            def update_task_status(self, task_id: str, status: str, **kwargs) -> None:
        """
        Update task status and other fields.
        
        Args:
            task_id (str): Task ID
            status (str): New status (PENDING, IN_PROGRESS, COMPLETED, FAILED)
            **kwargs: Additional fields to update
        """
        try:
            update_expression = "SET #status = :status, updated_at = :updated_at"
            expression_attribute_names = {'#status': 'status'}
            expression_attribute_values = {
                ':status': status,
                ':updated_at': datetime.utcnow().isoformat()
            }
            
            # Add additional fields to update
            for key, value in kwargs.items():
                if key not in ['task_id']:  # Skip primary key
                    update_expression += f", {key} = :{key}"
                    expression_attribute_values[f":{key}"] = self._convert_to_dynamodb_format(value)
            
            self.table.update_item(
                Key={'task_id': task_id},
                UpdateExpression=update_expression,
                ExpressionAttributeNames=expression_attribute_names,
                ExpressionAttributeValues=expression_attribute_values
            )
            
            logger.info(f"Updated task {task_id} status to {status}")
            
        except ClientError as e:
            logger.error(f"Error updating task {task_id}: {e}")
            raise
    
            def increment_processed_records(self, task_id: str, increment: int = 1) -> None:
        """
        Increment the processed records count for a task.
        
        Args:
            task_id (str): Task ID
            increment (int): Number to increment by
        """
        try:
            self.table.update_item(
                Key={'task_id': task_id},
                UpdateExpression="ADD processed_records :inc SET updated_at = :updated_at",
                ExpressionAttributeValues={
                    ':inc': increment,
                    ':updated_at': datetime.utcnow().isoformat()
                }
            )
            
            logger.debug(f"Incremented processed records for task {task_id} by {increment}")
            
        except ClientError as e:
            logger.error(f"Error incrementing processed records for task {task_id}: {e}")
            raise
    
            def increment_failed_records(self, task_id: str, increment: int = 1) -> None:
        """
        Increment the failed records count for a task.
        
        Args:
            task_id (str): Task ID
            increment (int): Number to increment by
        """
        try:
            self.table.update_item(
                Key={'task_id': task_id},
                UpdateExpression="ADD failed_records :inc SET updated_at = :updated_at",
                ExpressionAttributeValues={
                    ':inc': increment,
                    ':updated_at': datetime.utcnow().isoformat()
                }
            )
            
            logger.debug(f"Incremented failed records for task {task_id} by {increment}")
            
        except ClientError as e:
            logger.error(f"Error incrementing failed records for task {task_id}: {e}")
            raise
    
            def get_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        """
        Get task details by task ID.
        
        Args:
            task_id (str): Task ID
            
        Returns:
            Optional[Dict]: Task details or None if not found
        """
        try:
            response = self.table.get_item(Key={'task_id': task_id})
            
            if 'Item' in response:
                # Convert Decimal objects to native Python types
                return self._convert_decimal(response['Item'])
            else:
                logger.warning(f"Task {task_id} not found")
                return None
                
        except ClientError as e:
            logger.error(f"Error getting task {task_id}: {e}")
            raise
    
            def set_task_error(self, task_id: str, error_message: str) -> None:
        """
        Set task status to FAILED with error message.
        
        Args:
            task_id (str): Task ID
            error_message (str): Error message
        """
        try:
            self.update_task_status(task_id, 'FAILED', error_message=error_message)
            logger.error(f"Task {task_id} failed: {error_message}")
            
        except ClientError as e:
            logger.error(f"Error setting task error for {task_id}: {e}")
            raise
    
            def update_task_with_chunks(self, task_id: str, total_chunks: int, estimated_total_records: int, chunks_metadata: list) -> None:
        """
        Update task with Step Functions chunk information.
        
        Args:
            task_id: Task identifier
            total_chunks: Total number of chunks to process
            estimated_total_records: Estimated total records in CSV
            chunks_metadata: List of chunk metadata
        """
        try:
            current_time = datetime.utcnow().isoformat()
            
            # Create chunk status mapping
            chunk_status = {}
            chunk_processed = {}
            chunk_failed = {}
            chunk_vectors = {}
            chunk_errors = {}
            
            for chunk in chunks_metadata:
                chunk_id = chunk['chunk_id']
                chunk_status[chunk_id] = 'PENDING'
                # Initialize other maps as empty for each chunk
                # These will be populated when chunks are processed
            
            self.table.update_item(
                Key={'task_id': task_id},
                UpdateExpression='SET #status = :status, updated_at = :updated_at, total_chunks = :total_chunks, estimated_total_records = :estimated_total_records, completed_chunks = :completed_chunks, failed_chunks = :failed_chunks, chunk_status = :chunk_status, chunk_processed = :chunk_processed, chunk_failed = :chunk_failed, chunk_vectors = :chunk_vectors, chunk_errors = :chunk_errors',
                ExpressionAttributeNames={'#status': 'status'},
                ExpressionAttributeValues={
                    ':status': 'INITIALIZING',
                    ':updated_at': current_time,
                    ':total_chunks': total_chunks,
                    ':estimated_total_records': estimated_total_records,
                    ':completed_chunks': 0,
                    ':failed_chunks': 0,
                    ':chunk_status': chunk_status,
                    ':chunk_processed': chunk_processed,
                    ':chunk_failed': chunk_failed,
                    ':chunk_vectors': chunk_vectors,
                    ':chunk_errors': chunk_errors
                }
            )
            
            logger.info(f"Updated task {task_id} with {total_chunks} chunks")
            
        except ClientError as e:
            logger.error(f"Error updating task with chunks for {task_id}: {e}")
            raise
    
            def update_chunk_status(self, task_id: str, chunk_id: str, status: str, error_message: str = None, 
                           processed_records: int = None, failed_records: int = None, vectors_upserted: int = None) -> None:
        """
        Update the status of a specific chunk.
        
        Args:
            task_id: Task identifier
            chunk_id: Chunk identifier
            status: New chunk status (PENDING, IN_PROGRESS, COMPLETED, FAILED)
            error_message: Optional error message for failed chunks
            processed_records: Number of records processed in this chunk
            failed_records: Number of records that failed in this chunk
            vectors_upserted: Number of vectors upserted to Pinecone
        """
        try:
            current_time = datetime.utcnow().isoformat()
            
            # Build update expression dynamically
            # Start with basic updates that always happen
            update_expression_parts = [
                'chunk_status.#chunk_id = :status',
                'updated_at = :updated_at'
            ]
            
            expression_attribute_names = {'#chunk_id': chunk_id}
            expression_attribute_values = {
                ':status': status,
                ':updated_at': current_time
            }
            
            # Now add the specific field updates directly (without map initializations)
            if error_message:
                update_expression_parts.append('chunk_errors.#chunk_id = :error')
                expression_attribute_values[':error'] = error_message
            
            if processed_records is not None:
                update_expression_parts.append('chunk_processed.#chunk_id = :processed')
                expression_attribute_values[':processed'] = processed_records
            
            if failed_records is not None:
                update_expression_parts.append('chunk_failed.#chunk_id = :failed')
                expression_attribute_values[':failed'] = failed_records
            
            if vectors_upserted is not None:
                update_expression_parts.append('chunk_vectors.#chunk_id = :vectors')
                expression_attribute_values[':vectors'] = vectors_upserted
            
            # Combine all parts into final expression
            update_expression = 'SET ' + ', '.join(update_expression_parts)
            
            self.table.update_item(
                Key={'task_id': task_id},
                UpdateExpression=update_expression,
                ExpressionAttributeNames=expression_attribute_names,
                ExpressionAttributeValues=expression_attribute_values
            )
            
            logger.debug(f"Updated chunk {chunk_id} status to {status} for task {task_id}")
            
        except ClientError as e:
            logger.error(f"Error updating chunk {chunk_id} status for {task_id}: {e}")
            # Don't raise here - chunk status update failures shouldn't fail the entire process

            def acquire_processing_lock(self, start_date: str, end_date: str, lambda_request_id: str) -> bool:
        """
        Acquire a lock for processing a specific date range to prevent concurrent processing.
        
        Args:
            start_date (str): Start date in YYYY-MM-DD format
            end_date (str): End date in YYYY-MM-DD format
            lambda_request_id (str): Lambda request ID for this invocation
            
        Returns:
            bool: True if lock acquired successfully, False if already locked
        """
        try:
            lock_key = f"{start_date}_{end_date}"
            current_time = datetime.utcnow().isoformat()
            
            # Try to acquire lock with conditional write
            self.lock_table.put_item(
                Item={
                    'lock_key': lock_key,
                    'lambda_request_id': lambda_request_id,
                    'acquired_at': current_time,
                    'start_date': start_date,
                    'end_date': end_date,
                    'ttl': int(datetime.utcnow().timestamp()) + 3600  # Lock expires in 1 hour
                },
                ConditionExpression='attribute_not_exists(lock_key)'
            )
            
            logger.info(f"Successfully acquired processing lock for {start_date} to {end_date} (request: {lambda_request_id})")
            return True
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'ConditionalCheckFailedException':
                # Lock already exists
                logger.warning(f"Processing lock already exists for {start_date} to {end_date}")
                try:
                    # Get existing lock info
                    response = self.lock_table.get_item(Key={'lock_key': lock_key})
                    if 'Item' in response:
                        existing_lock = response['Item']
                        logger.warning(f"Existing lock held by request {existing_lock.get('lambda_request_id')} since {existing_lock.get('acquired_at')}")
                except:
                    pass
                return False
            else:
                logger.error(f"Error acquiring processing lock: {e}")
                raise

            def release_processing_lock(self, start_date: str, end_date: str, lambda_request_id: str) -> None:
        """
        Release the processing lock for a specific date range.
        
        Args:
            start_date (str): Start date in YYYY-MM-DD format
            end_date (str): End date in YYYY-MM-DD format
            lambda_request_id (str): Lambda request ID that holds the lock
        """
        try:
            lock_key = f"{start_date}_{end_date}"
            
            # Only delete if we own the lock
            self.lock_table.delete_item(
                Key={'lock_key': lock_key},
                ConditionExpression='lambda_request_id = :request_id',
                ExpressionAttributeValues={':request_id': lambda_request_id}
            )
            
            logger.info(f"Successfully released processing lock for {start_date} to {end_date} (request: {lambda_request_id})")
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'ConditionalCheckFailedException':
                logger.warning(f"Cannot release lock - not owned by request {lambda_request_id}")
            else:
                logger.error(f"Error releasing processing lock: {e}")
                # Don't raise - this is cleanup, shouldn't fail the main process
    
            def update_task_metadata(self, task_id: str, metadata: Dict[str, Any]) -> None:
        """
        Update task with metadata fields (for migration tracking).
        
        Args:
            task_id (str): Task ID
            metadata (Dict): Metadata fields to update
        """
        try:
            update_expression_parts = []
            expression_attribute_names = {}
            expression_attribute_values = {
                ':updated_at': datetime.utcnow().isoformat()
            }
            
            for key, value in metadata.items():
                if key == 'task_id':  # Skip primary key
                    continue
                
                # Handle nested fields (e.g., migration_progress.phase)
                if '.' in key:
                    parts = key.split('.')
                    safe_key = parts[0]
                    nested_path = '.'.join(f'#{p}' for p in parts[1:])
                    full_path = f'#{safe_key}.{nested_path}'
                    
                    for part in parts:
                        expression_attribute_names[f'#{part}'] = part
                else:
                    safe_key = f'#{key}'
                    full_path = safe_key
                    expression_attribute_names[safe_key] = key
                
                value_key = f':{key.replace(".", "_")}'
                update_expression_parts.append(f'{full_path} = {value_key}')
                expression_attribute_values[value_key] = self._convert_to_dynamodb_format(value)
            
            if not update_expression_parts:
                return
            
            update_expression = 'SET ' + ', '.join(update_expression_parts) + ', updated_at = :updated_at'
            
            self.table.update_item(
                Key={'task_id': task_id},
                UpdateExpression=update_expression,
                ExpressionAttributeNames=expression_attribute_names if expression_attribute_names else None,
                ExpressionAttributeValues=expression_attribute_values
            )
            
            logger.debug(f"Updated task {task_id} metadata: {list(metadata.keys())}")
            
        except ClientError as e:
            logger.error(f"Error updating task metadata for {task_id}: {e}")
            raise
    
            def get_migration_tasks(self, status: Optional[str] = None) -> list:
        """
        Get all migration tasks, optionally filtered by status.
        
        Args:
            status (str): Optional status filter
            
        Returns:
            list: List of migration tasks
        """
        try:
            if status:
                response = self.table.scan(
                    FilterExpression='task_type = :type AND #status = :status',
                    ExpressionAttributeNames={'#status': 'status'},
                    ExpressionAttributeValues={
                        ':type': 'namespace_migration',
                        ':status': status
                    }
                )
            else:
                response = self.table.scan(
                    FilterExpression='task_type = :type',
                    ExpressionAttributeValues={':type': 'namespace_migration'}
                )
            
            tasks = response.get('Items', [])
            
            # Handle pagination
            while 'LastEvaluatedKey' in response:
                if status:
                    response = self.table.scan(
                        FilterExpression='task_type = :type AND #status = :status',
                        ExpressionAttributeNames={'#status': 'status'},
                        ExpressionAttributeValues={
                            ':type': 'namespace_migration',
                            ':status': status
                        },
                        ExclusiveStartKey=response['LastEvaluatedKey']
                    )
                else:
                    response = self.table.scan(
                        FilterExpression='task_type = :type',
                        ExpressionAttributeValues={':type': 'namespace_migration'},
                        ExclusiveStartKey=response['LastEvaluatedKey']
                    )
                tasks.extend(response.get('Items', []))
            
            # Convert Decimal objects
            return [self._convert_decimal(task) for task in tasks]
            
        except ClientError as e:
            logger.error(f"Error getting migration tasks: {e}")
            raise
    
            def create_background_task(self, task_type: str, parameters: Dict[str, Any], 
                             priority: str = 'normal', celery_task_id: str = None) -> str:
        """
        Create a background task for Celery processing.
        
        Args:
            task_type: Type of background task (metabase_processing, s3_csv_processing, etc.)
            parameters: Task parameters and configuration
            priority: Task priority (low, normal, high)
            celery_task_id: Celery task ID for tracking
            
        Returns:
            str: Task ID
        """
        try:
            task_id = str(uuid.uuid4())
            timestamp = datetime.utcnow().isoformat()
            
            item = {
                'task_id': task_id,
                'status': 'QUEUED',
                'task_type': task_type,
                'priority': priority,
                'parameters': parameters,
                'celery_task_id': celery_task_id,
                'created_at': timestamp,
                'updated_at': timestamp,
                'queued_at': timestamp,
                'total_records': 0,
                'processed_records': 0,
                'failed_records': 0,
                'error_message': None,
                'progress_percentage': 0,
                'estimated_completion_time': None
            }
            
            self.table.put_item(Item=item)
            logger.info(f"Created background task {task_id} with type {task_type}")
            
            return task_id
            
        except ClientError as e:
            logger.error(f"Error creating background task: {e}")
            raise
    
            def update_task_progress(self, task_id: str, processed_records: int, 
                           total_records: int = None, status: str = None, 
                           metadata: Dict[str, Any] = None) -> None:
        """
        Update task progress with detailed tracking.
        
        Args:
            task_id: Task ID
            processed_records: Current number of processed records
            total_records: Total records to process (optional)
            status: Updated status (optional)
            metadata: Additional metadata to store
        """
        try:
            current_time = datetime.utcnow().isoformat()
            
            update_expression_parts = ['updated_at = :updated_at', 'processed_records = :processed']
            expression_attribute_values = {
                ':updated_at': current_time,
                ':processed': processed_records
            }
            
            if total_records is not None:
                update_expression_parts.append('total_records = :total')
                expression_attribute_values[':total'] = total_records
                
                # Calculate progress percentage
                progress = (processed_records / total_records) * 100 if total_records > 0 else 0
                update_expression_parts.append('progress_percentage = :progress')
                expression_attribute_values[':progress'] = self._convert_to_dynamodb_format(progress)
            
            if status:
                update_expression_parts.append('#status = :status')
                expression_attribute_values[':status'] = status
                
                # Track status change timestamps
                if status == 'IN_PROGRESS':
                    update_expression_parts.append('started_at = :started_at')
                    expression_attribute_values[':started_at'] = current_time
                elif status in ['COMPLETED', 'FAILED', 'CANCELLED']:
                    update_expression_parts.append('completed_at = :completed_at')
                    expression_attribute_values[':completed_at'] = current_time
            
            if metadata:
                for key, value in metadata.items():
                    if key not in ['task_id']:  # Skip primary key
                        update_expression_parts.append(f'{key} = :{key}')
                        expression_attribute_values[f':{key}'] = self._convert_to_dynamodb_format(value)
            
            update_expression = 'SET ' + ', '.join(update_expression_parts)
            expression_attribute_names = {'#status': 'status'} if status else None
            
            self.table.update_item(
                Key={'task_id': task_id},
                UpdateExpression=update_expression,
                ExpressionAttributeNames=expression_attribute_names,
                ExpressionAttributeValues=expression_attribute_values
            )
            
            logger.debug(f"Updated task {task_id} progress: {processed_records} processed")
            
        except ClientError as e:
            logger.error(f"Error updating task progress for {task_id}: {e}")
            raise
    
            def get_tasks_by_status(self, status: str, task_type: str = None, limit: int = 100) -> List[Dict[str, Any]]:
        """
        Get tasks by status, optionally filtered by type.
        
        Args:
            status: Task status to filter by
            task_type: Optional task type filter
            limit: Maximum number of tasks to return
            
        Returns:
            List of task records
        """
        try:
            if task_type:
                response = self.table.scan(
                    FilterExpression='#status = :status AND task_type = :task_type',
                    ExpressionAttributeNames={'#status': 'status'},
                    ExpressionAttributeValues={
                        ':status': status,
                        ':task_type': task_type
                    },
                    Limit=limit
                )
            else:
                response = self.table.scan(
                    FilterExpression='#status = :status',
                    ExpressionAttributeNames={'#status': 'status'},
                    ExpressionAttributeValues={':status': status},
                    Limit=limit
                )
            
            tasks = response.get('Items', [])
            return [self._convert_decimal(task) for task in tasks]
            
        except ClientError as e:
            logger.error(f"Error getting tasks by status {status}: {e}")
            raise
    
            def get_active_tasks(self, limit: int = 100) -> List[Dict[str, Any]]:
        """
        Get all active (non-completed) tasks.
        
        Args:
            limit: Maximum number of tasks to return
            
        Returns:
            List of active task records
        """
        try:
            response = self.table.scan(
                FilterExpression='#status IN (:queued, :pending, :in_progress, :processing_chunks, :initializing)',
                ExpressionAttributeNames={'#status': 'status'},
                ExpressionAttributeValues={
                    ':queued': 'QUEUED',
                    ':pending': 'PENDING',
                    ':in_progress': 'IN_PROGRESS',
                    ':processing_chunks': 'PROCESSING_CHUNKS',
                    ':initializing': 'INITIALIZING'
                },
                Limit=limit
            )
            
            tasks = response.get('Items', [])
            return [self._convert_decimal(task) for task in tasks]
            
        except ClientError as e:
            logger.error(f"Error getting active tasks: {e}")
            raise
    
            def cancel_task(self, task_id: str, reason: str = None) -> None:
        """
        Cancel a task and update its status.
        
        Args:
            task_id: Task ID to cancel
            reason: Reason for cancellation
        """
        try:
            current_time = datetime.utcnow().isoformat()
            
            update_data = {
                'status': 'CANCELLED',
                'updated_at': current_time,
                'completed_at': current_time
            }
            
            if reason:
                update_data['cancellation_reason'] = reason
            
            self.update_task_status(task_id, 'CANCELLED', **update_data)
            logger.info(f"Task {task_id} cancelled. Reason: {reason or 'No reason provided'}")
            
        except ClientError as e:
            logger.error(f"Error cancelling task {task_id}: {e}")
            raise
    
            def get_task_statistics(self, time_period_hours: int = 24) -> Dict[str, Any]:
        """
        Get task statistics for a given time period.
        
        Args:
            time_period_hours: Hours to look back for statistics
            
        Returns:
            Dict with task statistics
        """
        try:
            # Calculate time range
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=time_period_hours)
            start_timestamp = start_time.isoformat()
            
            response = self.table.scan(
                FilterExpression='updated_at >= :start_time',
                ExpressionAttributeValues={':start_time': start_timestamp}
            )
            
            tasks = response.get('Items', [])
            
            # Handle pagination
            while 'LastEvaluatedKey' in response:
                response = self.table.scan(
                    FilterExpression='updated_at >= :start_time',
                    ExpressionAttributeValues={':start_time': start_timestamp},
                    ExclusiveStartKey=response['LastEvaluatedKey']
                )
                tasks.extend(response.get('Items', []))
            
            # Convert and calculate statistics
            tasks = [self._convert_decimal(task) for task in tasks]
            
            stats = {
                'time_period_hours': time_period_hours,
                'total_tasks': len(tasks),
                'status_counts': {},
                'type_counts': {},
                'total_records_processed': 0,
                'total_records_failed': 0,
                'average_processing_time_minutes': 0,
                'success_rate_percentage': 0
            }
            
            processing_times = []
            
            for task in tasks:
                # Status counts
                status = task.get('status', 'unknown')
                stats['status_counts'][status] = stats['status_counts'].get(status, 0) + 1
                
                # Type counts
                task_type = task.get('task_type', 'unknown')
                stats['type_counts'][task_type] = stats['type_counts'].get(task_type, 0) + 1
                
                # Record counts
                stats['total_records_processed'] += task.get('processed_records', 0)
                stats['total_records_failed'] += task.get('failed_records', 0)
                
                # Processing time
                if task.get('started_at') and task.get('completed_at'):
                    try:
                        started = datetime.fromisoformat(task['started_at'].replace('Z', '+00:00'))
                        completed = datetime.fromisoformat(task['completed_at'].replace('Z', '+00:00'))
                        duration = (completed - started).total_seconds() / 60  # minutes
                        processing_times.append(duration)
                    except:
                        pass
            
            # Calculate averages
            if processing_times:
                stats['average_processing_time_minutes'] = sum(processing_times) / len(processing_times)
            
            total_records = stats['total_records_processed'] + stats['total_records_failed']
            if total_records > 0:
                stats['success_rate_percentage'] = (stats['total_records_processed'] / total_records) * 100
            
            return stats
            
        except ClientError as e:
            logger.error(f"Error getting task statistics: {e}")
            raise
    
            def cleanup_completed_tasks(self, retention_days: int = 7) -> int:
        """
        Clean up completed tasks older than retention period.
        
        Args:
            retention_days: Number of days to retain completed tasks
            
        Returns:
            Number of tasks deleted
        """
        try:
            cutoff_date = datetime.utcnow() - timedelta(days=retention_days)
            cutoff_timestamp = cutoff_date.isoformat()
            
            # Find old completed tasks
            response = self.table.scan(
                FilterExpression='#status IN (:completed, :failed, :cancelled) AND updated_at < :cutoff',
                ExpressionAttributeNames={'#status': 'status'},
                ExpressionAttributeValues={
                    ':completed': 'COMPLETED',
                    ':failed': 'FAILED',
                    ':cancelled': 'CANCELLED',
                    ':cutoff': cutoff_timestamp
                }
            )
            
            old_tasks = response.get('Items', [])
            
            # Handle pagination
            while 'LastEvaluatedKey' in response:
                response = self.table.scan(
                    FilterExpression='#status IN (:completed, :failed, :cancelled) AND updated_at < :cutoff',
                    ExpressionAttributeNames={'#status': 'status'},
                    ExpressionAttributeValues={
                        ':completed': 'COMPLETED',
                        ':failed': 'FAILED',
                        ':cancelled': 'CANCELLED',
                        ':cutoff': cutoff_timestamp
                    },
                    ExclusiveStartKey=response['LastEvaluatedKey']
                )
                old_tasks.extend(response.get('Items', []))
            
            # Delete in batches
            deleted_count = 0
            batch_size = 25  # DynamoDB batch write limit
            
            for i in range(0, len(old_tasks), batch_size):
                batch = old_tasks[i:i + batch_size]
                
                with self.table.batch_writer() as batch_writer:
                    for task in batch:
                        batch_writer.delete_item(Key={'task_id': task['task_id']})
                        deleted_count += 1
            
            logger.info(f"Cleaned up {deleted_count} old task records")
            return deleted_count
            
        except ClientError as e:
            logger.error(f"Error cleaning up completed tasks: {e}")
            raise